{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad115d3c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62056a25",
   "metadata": {},
   "source": [
    "# MNE-BIDS pipeline\n",
    ">\n",
    "#### This document will serve as a step-by-step guide to walk you through transforming your EEG dataset into one compliant with the BIDS format, using MNE-BIDS.\n",
    "\n",
    "##### It will walk you through:\n",
    "-> General information\n",
    ">\n",
    "-> Downloading/formatting your dataset\n",
    ">\n",
    "-> Setting up your coding environment\n",
    ">\n",
    "-> Transforming your EEG data to BIDS\n",
    ">\n",
    "-> Adapting your BIDS dataset to include relevant metadata\n",
    ">\n",
    "-> Validating your BIDS dataset\n",
    ">\n",
    "-> Citing MNE-BIDS\n",
    ">\n",
    "-> Adapting the code to iterate through all participants "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aaaf8d",
   "metadata": {},
   "source": [
    "The pipeline can be separated into two main sections, 'Transforming EEG data to BIDS', and 'Adapting your BIDS dataset to include relevant metadata'. The first of these will produce a valid BIDS dataset, containing all BIDS-required fields. However, the second section is required to generate a dataset that includes _all_ of the meaningful metadata collected from your study, as a lot of information that may be important for your dataset cannot be handled by MNE-BIDS.\n",
    "\n",
    "Thus, to generate a valid _and_ complete BIDS dataset, you should complete both main sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc987f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eba06a",
   "metadata": {},
   "source": [
    "## What is MNE?\n",
    "> MNE is an open source python package for working with EEG and MEG data, which serves to facilitate the exploration, visualisation and analysis of neuroimaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca1136",
   "metadata": {},
   "source": [
    "## What is BIDS?\n",
    "> BIDS (Brain Imaging Data Structure) is a simple method of organising neuroimaging data that is easy to adopt and promotes standardisation across neuroimaging experiments. \n",
    "\n",
    "This standardisation is important as it allows other researchers to easily understand and work with your data, fostering collaboration and openness, and better adhering to FAIR principles, ensuring data is Findable, Accessible, Interoperable and Reusable. Additionally, many software packages and databases (such as [OpenNeuro.org](https://openneuro.org/)) prefer or require BIDS formatted datasets, so formatting your data in this way makes publication and curation of data much simpler!\n",
    "\n",
    ">\n",
    "> It involves a hierarchical folder organisation structure, with four main levels:\n",
    ">\n",
    "\n",
    "-> project\n",
    "\n",
    "---> subject\n",
    "\n",
    "----->  session\n",
    "\n",
    "-------> datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45baf688",
   "metadata": {},
   "source": [
    "## SO, MNE-BIDS...?\n",
    "> Is a processing pipeline that uses MNE-python tools to generate BIDS compliant datasets!\n",
    ">\n",
    "If you don't currently have MNE-BIDS installed, please refer to their official [website](https://mne.tools/mne-bids-pipeline/stable/getting_started/install.html) to do so before beginning this walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801b2e4c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54b20a",
   "metadata": {},
   "source": [
    "#### To some, the contents of this document may appear incredibly complex. We understand that without prior experience, reaching BIDS-compliancy can be incredibly difficult, but we also know that its advantages are striking! As such, we will do our best to make this tutorial in-depth and beginner friendly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a05879",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588cc48",
   "metadata": {},
   "source": [
    "# What versions will this document use?\n",
    "\n",
    "#### - MNE version: 1.9.0\n",
    "#### - BIDS version: 1.10.0\n",
    "#### - MNE-BIDS version: 0.16.0\n",
    "\n",
    "Note: The document is also tailored towards windows operating systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866c154",
   "metadata": {},
   "source": [
    "# Expected Proficiencies\n",
    "> #### Prior to using this pipeline, a certain level of understanding/ skill is expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572f6dd",
   "metadata": {},
   "source": [
    "This entails:\n",
    "- Some knowledge of python (to understand and implement the present code), although this will be explained throughout.\n",
    "- An understanding of what a BIDS formatted dataset should include and how it should look (for checking the dataset has converted correctly).\n",
    "  > This information can be found on the [BIDS website](https://bids.neuroimaging.io/getting_started/index.html).\n",
    "- Familiarity with your EEG dataset and its associated metadata (to ensure all important information is present post-conversion and add any that is missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96f5df",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f30e7",
   "metadata": {},
   "source": [
    "# 1. Downloading data\n",
    "> #### Collecting the EEG dataset necessary to run through this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb325c6",
   "metadata": {},
   "source": [
    "In order to complete this pipeline, you will first need some EEG data. If you intend to run this pipeline using your pre-existing dataset, you can simply move onto the next step. If you don't have any EEG data to test this process on, we suggest downloading the [EEG Motor Movement/Imagery Dataset](https://physionet.org/content/eegmmidb/1.0.0/) from the [Physiobank Database](https://physionet.org/data/). This document will use this as example data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5a541",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c438d4",
   "metadata": {},
   "source": [
    "# 2. Data formatting\n",
    "> #### This pipeline's data format expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28464a6e",
   "metadata": {},
   "source": [
    "This pipeline is curated to work with EDF (European Data Format) formatted datasets, however MNE is capable of handling a variety of formats. \n",
    ">  If your data is currently in a different format, you will need to use a slightly different section of code when reading in your data (step 6). For guidance on this, refer to MNE's documentation on [importing data from EEG devices](https://mne.tools/stable/auto_tutorials/io/20_reading_eeg_data.html#sphx-glr-auto-tutorials-io-20-reading-eeg-data-py) for guidance. \n",
    ">\n",
    "The pipeline will also write the dataset into the EDF format in step 7 as recommended by BIDS. If you require a different output format, you may edit the `format` parameter of `write_raw_bids` using [MNE's guidance](https://mne.tools/mne-bids/stable/generated/mne_bids.write_raw_bids.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bc51d",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd888951",
   "metadata": {},
   "source": [
    "## Creating a virtual environment\n",
    "Before you can begin your conversion, it will be useful to create a virtual environment, which allows you to store packages in a self-contained space that won't affect any of your other coding projects.\n",
    "\n",
    "Using this, we can also install all of the packages from a different virtual environment to ensure you have all the same things installed, with the exact same versions and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fc7aa",
   "metadata": {},
   "source": [
    "First, let's create a directory for our project (using the first line of code), then move into that (using the second line of code). These steps __must__ be completed using your computer's terminal; for windows devices, this will be 'Windows PowerShell'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a directory for the current project\n",
    "mkdir mne-bids-pipeline\n",
    "# Moving into the newly created directory\n",
    "cd mne-bids-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2a5d8",
   "metadata": {},
   "source": [
    "Then, using the venv module, we will create a virtual environment, which will be contained in the sub-directory '.venv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ee652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a virtual environment using the venv module (with the sub-directory name '.venv')\n",
    "python -m venv .venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9bd10e",
   "metadata": {},
   "source": [
    "Now, lets activate the virtual environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949808aa",
   "metadata": {},
   "outputs": [],
   "source": [
    ".venv/Scripts/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453872b8",
   "metadata": {},
   "source": [
    "Now, you just have to install the requirements for this pipeline! \n",
    "\n",
    "This can be done using uv (which is generally a more powerful and complete tool) or pip (which is slightly simpler to use). \n",
    "\n",
    "[EXPLAIN WHAT UV AND PIP ARE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44d451",
   "metadata": {},
   "source": [
    "Using uv:\n",
    "- You must first [install uv](https://docs.astral.sh/uv/getting-started/installation/).\n",
    "- You should also [install git](https://github.com/git-guides/install-git)\n",
    "- Then, you must [clone the repo](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) this document is stored in (['RosettaState'](https://github.com/ubdbra001/RosettaState)) to your local machine. \n",
    "- From here, you must navigate to the cloned repo's directory. This can be done using windows powershell by typing cd, then pasting in the folder path (e.g. `cd C:\\Users\\Ariana\\github\\RosettaState`).\n",
    "- Finally, you can type `uv sync` into the same terminal to install all required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd <your_cloned_repo's_folder_path>\n",
    "\n",
    "uv sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658bc68c",
   "metadata": {},
   "source": [
    "Using pip:\n",
    "- First, you will need to [install python](https://www.python.org/downloads/windows/). This should also install pip.\n",
    "- Next, you must locate the 'requirements.txt' file within this document's repo, ['RosettaState'](https://github.com/ubdbra001/RosettaState/tree/MNE-BIDS-pipeline-post-feedback).\n",
    "- From here, you can install the dependencies from 'requirements.txt' by typing the code below into the powershell terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dependencies from requirements.txt: \n",
    "python -m pip install --requirement requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a62ce0",
   "metadata": {},
   "source": [
    "# 3. Importing the necessary tools\n",
    ">### To begin, we will need to import all the tools necessary for converting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc978cc5",
   "metadata": {},
   "source": [
    "This first section of code will import tools that allow us to work with the file paths and simplify the method of handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222527de",
   "metadata": {},
   "source": [
    "Next, we need to `import MNE`, a python package for working with EEG and MEG data, and some associated tools that we will use here. \n",
    ">\n",
    "From `mne_bids`, we are importing:\n",
    "- `BIDSPath`:\n",
    "A tool for creating a BIDS formatted file path\n",
    "- `print_dir_tree`:\n",
    "A tool for presenting the contents of a folder in a 'tree' view\n",
    "- `write_raw_bids`:\n",
    "A tool for saving EEG data into BIDS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "from mne_bids import BIDSPath, print_dir_tree, write_raw_bids, make_dataset_description, update_sidecar_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c40bee",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd272f5d",
   "metadata": {},
   "source": [
    "# 4. Finding the data\n",
    ">### After completing our imports, we need to find the EEG data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbeb4d",
   "metadata": {},
   "source": [
    "In the code below, the first line specifies where the folders and sub-folders for the data can be found. These should include your EEG data and any additional information (metadata). \n",
    ">\n",
    "You should modify this to include your own file pathway: `data_dir = Path(r\"___your file pathway____\")`. This should be the file containing your task files, or the highest file level containing your dataset and no external (dataset-unrelated) files.\n",
    "> Here, the `r` (raw) ensures the file location is read as is, and that the backslashes don't get interpreted as special characters, so don't break up the text.\n",
    ">\n",
    "The line below this prints a visualisation of the first sub-folders within (using the `print_dir_tree` tool!). \n",
    "> You may have 1 or more of these, depending on how much EEG data you wish to make BIDS compliant. Each of these should contain EEG data from one specific task type, including data from each participant and any associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ad766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the file path to your data's location\n",
    "data_dir = Path(r\"C:\\N8_internship_code\\Motor_Imaging_Dataset\")\n",
    "print_dir_tree(data_dir, max_depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0500035",
   "metadata": {},
   "source": [
    "This next section lists the file paths for the sub-folders we just visualised and adds them to the list 'children'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab5a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "children = [child for child in data_dir.iterdir()]\n",
    "children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef6ebe",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35b5456",
   "metadata": {},
   "source": [
    "# 5. Selecting specific files\n",
    ">### Let's specify the files we want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd7eb1",
   "metadata": {},
   "source": [
    "Here, the first line serves to identify which of the two files we want to write into BIDS format (note: in python, the first index is given a value of 0). If you have multiple files, each time you run through this you should change the number at the end to match the file you are wanting to adapt. \n",
    ">\n",
    "The second line lists all the files in the specified subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to match the file number\n",
    "dir_number = 5\n",
    "files = [file for file in children[dir_number].iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1342cd1",
   "metadata": {},
   "source": [
    "This sets the first file in the folder to the variable `file_path`, then prints this. \n",
    ">\n",
    "Even when completing multiple iterations (for more than one dataset), the value should NOT be changed from 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da3668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = files[1]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bffe158",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932801a0",
   "metadata": {},
   "source": [
    "# 6. Reading/ specifying the data\n",
    ">### Now we've completed our preparations, lets compile our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae205c9",
   "metadata": {},
   "source": [
    "Here, we are reading the EEG data from the previously selected file path to the `data` variable.\n",
    ">\n",
    "As previously mentioned, the current code is tailored to EDF formatted datasets and won't work with any other formats. As such, you must use a slightly different line of code depending on the format of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_edf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d40778",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ecfb8e",
   "metadata": {},
   "source": [
    "### In this section we will also specify some important metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37101bce",
   "metadata": {},
   "source": [
    "This process will involve writing information to the 'info' dictionary, which holds all of the metadata for the dataset. Not all aspects of this can be written to, but a few can.\n",
    "\n",
    "So first, we need to import a few extra tools to help with this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0681c",
   "metadata": {},
   "source": [
    "Next, fill in the value section of the `key:value` pairs below to match the datset's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the EEG headset\n",
    "raw.info[\"device_info\"] = {\n",
    "    \"type\": \"eeg\",\n",
    "    \"model\": \"EasyCap 64-channel cap\",\n",
    "    \"serial\": \"1234567\",\n",
    "    }\n",
    "\n",
    "# The line frequency of the data (in hertz)\n",
    "raw.info[\"line_freq\"] = 50\n",
    "\n",
    "# A description of the recording\n",
    "raw.info[\"description\"] = \"A motor imaging dataset...\"\n",
    "\n",
    "# The name of the experimenter\n",
    "raw.info[\"experimenter\"] = \"John Doe\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b339fd3",
   "metadata": {},
   "source": [
    "If you know your dataset has any broken or noisy channels, you may enter these into the list below (each within a set of double quotation marks) to flag them within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222fabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of 'bad' (noisy or broken) channels, by name\n",
    "raw.info[\"bads\"] = [\"__\", \"__\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456dfc6",
   "metadata": {},
   "source": [
    "Then, enter in the required information for the subject info section using the same method. \n",
    "\n",
    "If the participant's birthdate is unknown, this can be calculated by inputting the date of measurement (YYYY/M/D) into the first line of code, then inputting the participant's age into the third line (years=___). The variable 'birthdate' can then be inputted as the value for the \"birthday\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db2545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the (approximate) birthdate of the participant based on the measurement date and age\n",
    "raw.set_meas_date(datetime(2015, 6, 7, tzinfo= timezone.utc))\n",
    "recording_date = raw.info[\"meas_date\"]\n",
    "Birthdate = recording_date - relativedelta(years=30)\n",
    "\n",
    "raw.info[\"subject_info\"] = {\n",
    "    \"id\": 1,\n",
    "    \"his_id\": \"sub-001\",\n",
    "    \"last_name\": \"Doe\",\n",
    "    \"first_name\": \"John\",\n",
    "    \"middle_name\": \"A\",\n",
    "    \"birthday\": Birthdate,\n",
    "    \"sex\": 2,\n",
    "    \"hand\": 1,\n",
    "    \"weight\": 70.0,\n",
    "    \"height\": 175.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0a64a",
   "metadata": {},
   "source": [
    "Finally, we will be inputting the montage for your dataset.\n",
    "\n",
    "The below code will list the standard montages that MNE-BIDS supports. From these, you should select the montage that applies to your dataset.\n",
    "\n",
    "MNE also supports the creation of your own dataset. Those that require this function may benefit from following MNE's guidelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "builtin_montages = mne.channels.get_builtin_montages(descriptions=True)\n",
    "for montage_name, montage_description in builtin_montages:\n",
    "    print(f\"{montage_name}: {montage_description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eac716",
   "metadata": {},
   "source": [
    "Next, you can input your montage name within the double quotation marks below to display it's information and visualise it as both a 2D and 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f136e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_montage = mne.channels.make_standard_montage(\"biosemi64\")\n",
    "\n",
    "# Printing montage information\n",
    "print(my_montage)\n",
    "\n",
    "# Visualising montage in 2D\n",
    "my_montage.plot()\n",
    "\n",
    "# Visualising montage in 3D\n",
    "fig = my_montage.plot(kind=\"3d\", show=False)  # 3D\n",
    "fig = fig.gca().view_init(azim=70, elev=15)  # set view angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfe45d",
   "metadata": {},
   "source": [
    "If you are happy with these visualisations, the montage can be written to the data using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.set_montage(my_montage, match_case=True, match_alias=False, on_missing='ignore', verbose=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b738e",
   "metadata": {},
   "source": [
    "#### Events\n",
    "It is also important that you specify any events in the EEG recording. Event information should be matched based on index throughout the input sections below. For each event, you should input their onset time (in seconds), their duration (in seconds), their description, their associated channels (empty entry = no specific channel associated), and their event id (dictionary of key value pairs). orig_time can also be edited, but uses measurement date (from 'info') as a default to sync annotations with raw data.\n",
    "\n",
    "Note: Any events with labels beginning with 'bad' or 'edge' will be ignored, and event id keys must be the same as event description names.\n",
    "\n",
    "The code will write all of these inputs to 'annotations' in the raw data, then turn these into events to be written to the BIDS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting time of annotations (in seconds) after 'orig time'\n",
    "onset = [0, 10.0, 30.5]  \n",
    "\n",
    "# Durations of the annotations (in seconds)\n",
    "duration = [0, 0.5, 1.0]\n",
    "\n",
    "# Descriptions for each annotation\n",
    "description = ['start', 'bad blink', 'stimulus']\n",
    "\n",
    "# List of channel names associated with the annotations (empty entries = no specific channel)\n",
    "ch_names = [[], ['O1', 'C3'], ['Pz']]\n",
    "\n",
    "# Determines the starting time of annotation acquisition, contains the timestamp as the first element and microseconds as the second element\n",
    "orig_time = None\n",
    "\n",
    "# Writing the inputs to 'annotations'\n",
    "annotations = mne.Annotations(onset, duration, description, orig_time=orig_time, ch_names=ch_names)\n",
    "raw.set_annotations(annotations)\n",
    "\n",
    "# The id associated with each event (via its description)\n",
    "event_id = {\n",
    "        'start': 1,\n",
    "        'bad blink': 2,\n",
    "        'stimulus': 3\n",
    "    }\n",
    "\n",
    "# Generates events from 'annotations'\n",
    "events, _ = mne.events_from_annotations(raw, event_id=event_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd970fd",
   "metadata": {},
   "source": [
    "This next section of code will create a new folder path for storing EEG data in BIDS format, then prints it out. \n",
    ">\n",
    "We recommend renaming your file to something more specific to your dataset, by switching out the text in the quotation marks. Attempt to avoiding using any spaces in the title to prevent possible later complications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_root = data_dir.parent/ \"Motor_Imaging_Example\"\n",
    "bids_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eaa835",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c23650",
   "metadata": {},
   "source": [
    "# 7. Writing the data\n",
    "> #### Let's write our selected data into BIDS format!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eae464",
   "metadata": {},
   "source": [
    "First, you should manually define the participant number/ subject id and task name for this dataset, setting them each to a variable as seen in the first two rows.\n",
    ">\n",
    "Then, using the `BIDSPath` tool we imported earlier, we will assign the subject, task and the folder path we just created to `bids_path`. \n",
    ">\n",
    "We will then use another imported tool, `write_raw_bids` to write the data (from the file path we defined earlier) into the new file path we created, linking it to the subject id and task type we outlined. The desired format of the output data is also outlined here `format=\"EEGLAB\"`.\n",
    "\n",
    "The last line will also write the previously generated 'events' and 'event_id' variables to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569683cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edit this information to match your data\n",
    "subject_id = \"S001\"\n",
    "task = \"task1\"\n",
    "\n",
    "bids_path = BIDSPath(subject=subject_id, task=task, root=bids_root)\n",
    "write_raw_bids(raw, bids_path, events=events, event_id=event_id, overwrite=True, allow_preload=True, format=\"EDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4597c",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9b874",
   "metadata": {},
   "source": [
    "### Now you have formatted your dataset to BIDS standards! \n",
    "#### Don't forget to repeat steps 4 and 5 for all of the file paths we found in step 3\n",
    ">\n",
    "## But hold on!\n",
    "#### Your BIDS formatted dataset isn't quite complete yet..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70d806",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f10e503",
   "metadata": {},
   "source": [
    "# Editing and checking your BIDS formatted dataset\n",
    "### The steps below will walk you through finding and editing some of the files in your new dataset, in order to make them BIDS-compliant. \n",
    "Each of these files should automatically include a large amount of information derived from your dataset and stored in BIDS format, however this may not always be completely accurate.\n",
    ">\n",
    "As such, the next steps will walk you through checking that your BIDS dataset is accurate, and how to adapt these files if necessary. Some of the file's items will be deemed required for a BIDS-compliant dataset, while others are recommended or merely optional. You __MUST__ ensure that the required elements are present and have correct data, and although not necessary, it will be beneficial for you to include as much additional data as possible, especially if it is important information for your dataset.\n",
    "\n",
    "> Don't forget to do these checks for all task types (all of the file pathways we found)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999b7d4",
   "metadata": {},
   "source": [
    "You can do this by navigating to the file path we assigned to the variable `bids_root` in step 4, then working through all of the files and investigating what is present/correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0828cf",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ecd93",
   "metadata": {},
   "source": [
    "# Editing different file formats\n",
    "Some of the following files will follow the .json format (Sidecar, Coordinate System, Dataset Description), others (Channels Description, Electrodes description) will be in the .tsv format, and a few will have a file in each format (Events, Participants).\n",
    "\n",
    "These file types are each edited via slightly different methods, so while .json files require no extra imports, to edit our .tsv files we must import the [pandas](https://pandas.pydata.org/pandas-docs/version/1.4/index.html) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e82cb5",
   "metadata": {},
   "source": [
    "Due to the differences in their display formats (text vs tabular), while .json files can be edited using a simple dictionary of key:value pairs, editing .tsv files requires a few different code functions. \n",
    "\n",
    "Those outlined in this document will walk you through:\n",
    "- Adding/ Editing a column\n",
    "- Editing the value of just one row\n",
    "- Removing a row\n",
    "- Adding a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the pandas library to edit .tsv files\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed1f56",
   "metadata": {},
   "source": [
    "# Assigning file pathways\n",
    "To ensure our next sections of code are as clean and easy to use as possible, we will be assigning key file pathway roots to variables. Later in the document we will use these to create file pathways to specific file locations.\n",
    "\n",
    "Here, you should:\n",
    "- Set the variable 'root' to the top-level folder of the BIDS dataset \n",
    "- Set the variable 'eeg_root' to the folder containing eeg data for the current subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cf5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'root' to the top-level folder of the BIDS dataset \n",
    "root = Path(r'c:\\N8_internship_code\\Motor_Imaging_Example')\n",
    "\n",
    "# Set 'eeg_root' to the folder containing eeg data for the current subject\n",
    "eeg_root = Path(r'C:\\N8_internship_code\\Motor_Imaging_Example\\sub-S001\\eeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09457884",
   "metadata": {},
   "source": [
    "Contents:\n",
    "- [Dataset description](#dataset-description)\n",
    "- [JSON files](#json)\n",
    "- [Phenotype Files](#phenotype)\n",
    "- [TSV files](#tsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0525166",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160aa3c",
   "metadata": {},
   "source": [
    "\n",
    "#### Despite also being a json file, the dataset description can't be edited using the same code. As such, any edits we hope to make to it must be done by following the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cacfd89",
   "metadata": {},
   "source": [
    "<a id=\"dataset-description\"></a>\n",
    "# Dataset Description\n",
    "> Edits to this file are incredibly important, as this outlines all of the general information about your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7799c",
   "metadata": {},
   "source": [
    "The code below will re-write the ENTIRE dataset description, overwriting any previous dataset description files.\n",
    "This file should describe the dataset in as much detail as possible, so you should attempt to include as much of the data outlined below as possible although BIDS only requires the presence of the 'necessary' information.\n",
    "\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. StudyName\n",
    "    2. BIDSVersion \n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    3. HEDVersion\n",
    "    4. DatasetType\n",
    "    5. DataLicense\n",
    "    6. Authors\n",
    "    7. GeneratedBy\n",
    "        - Name\n",
    "        - Version\n",
    "        - Container\n",
    "        - Type\n",
    "        - Tag\n",
    "    8. SourceDatasets\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    9. Acknowledgements\n",
    "    10. HowToAcknowledge\n",
    "    11. Funding\n",
    "    12. EthicsApprovals\n",
    "    13. ReferencesAndLinks\n",
    "    14 Doi\n",
    "Note: `BIDS version` will be automatically included in the data file once the code is run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1056b7",
   "metadata": {},
   "source": [
    "Once you have decided on the information you wish to include, you can append the code below, changing the information in quotation marks to your dataset's information.\n",
    ">\n",
    "Any that you don't intend on including should be written as `<item>=None`, just as `acknowledgements` is below. This will skip that item, preventing its inclusion in the file. \n",
    "> This code will overwrite any 'dataset description' file previously generated. This can be changed by changing `overwrite=True` to `overwrite=False`. \n",
    ">\n",
    "- Note: Doi must be written in the format: `doi:<insert_doi>`.\n",
    "\n",
    "> An example output file can be found within the [BIDS documentation](https://bids-specification.readthedocs.io/en/stable/modality-agnostic-files.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebb7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataset description JSON file\n",
    "# Will overwrite any existing dataset_description.json file in the root of the BIDS directory\n",
    "make_dataset_description(\n",
    "    path=bids_root,\n",
    "    name=\"EEG Motor Movement/ Imagery Dataset\", \n",
    "    hed_version=\"1\",\n",
    "    dataset_type='raw',\n",
    "    data_license=\"CCO\",\n",
    "    authors=[\"John Doe\", \"Jane Doe\"],\n",
    "    generated_by=[\n",
    "        {\n",
    "            \"Name\": \"MNE-BIDS\",\n",
    "            \"Version\": \"0.14\",\n",
    "            \"Description\": \"Used to convert MEG data into BIDS format.\"\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"MNE-Python\",\n",
    "            \"Version\": \"1.6.1\",\n",
    "            \"Description\": \"Used for MEG preprocessing and analysis.\"\n",
    "        }\n",
    "    ],\n",
    "    source_datasets=[\n",
    "        {\n",
    "            \"URL\": \"https://example.com/source_dataset\",\n",
    "            \"DOI\": \"10.1234/example.doi\",\n",
    "        }],\n",
    "    acknowledgements=None,\n",
    "    how_to_acknowledge=\"Cite (Doe et al., 2025) when using this dataset\",\n",
    "    funding=[\"The NHS\", \"The Uk government\"],\n",
    "    ethics_approvals=\"Ethical approval was granted by the University of ___ School of Psychology Ethics committee\",\n",
    "    references_and_links=\"https://mne.tools/mne-bids/stable/whats_new_previous_releases.html\",\n",
    "    doi=\"doi:https://doi.org/10.1016/j.tins.2017.02.004\",\n",
    "            overwrite=True,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2770038",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0c7fe",
   "metadata": {},
   "source": [
    "<a id=\"json\"></a>\n",
    "## Manually updating an element in a JSON file:\n",
    "\n",
    "Now that our dataset description file is up to date, we can work on editing the rest of our json files (if necessary). Each file uses similar code, with just a few key sections requiring adaptation.\n",
    "\n",
    "Below the code, information on all of the files and their list of BIDS items, organised on priority. These lists should be used to compare against your files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269e8be",
   "metadata": {},
   "source": [
    "To begin, you must change the `subject=` and `task=`, `suffix=` and `extension=` sections of `bids_path1` to match your chosen file's name. (For example, this line in the code below relates to the file 'sub-S001_task-task1_eeg.json').\n",
    "> Due to differences in file location, the participants.json file requires the input `None` for the `subject=`, `task=` and `datatype=` sections and the coordsystem.json file requires an extra variable, `space=` to be inputted while `task=` must be `None`. This refers to the coordinate system being used, e.g.'CapTrak'. [see examples below]\n",
    "\n",
    "Once these are re-defined, you can update one or more aspect(s) of the sidecar using the `entries = {}` dictionary. This accepts `key:value` pairs, separated by colons (:), wherein single quotation marks ('') indicate a variable name, while double quotation marks (\"\") indicate it's data entry.\n",
    ">\n",
    "##### The code below will display an example of a few formats the key-value pairs can present in, such as:\n",
    "__Numerical__\n",
    "    - A key-value pair where the value is a number (int/float).\n",
    ">\n",
    "__Written__\n",
    "    - A key-value pair where the value is a string (text).\n",
    ">\n",
    "__Nested dictionary (1 level)__\n",
    "    - A key-value pair where the value is a dictionary containing key-value pairs.\n",
    ">\n",
    "__Nested dictionary (2+ levels)__\n",
    "    - A key-value pair where the value is a dictionary that contains one or more dictionaries.\n",
    ">\n",
    "\n",
    "#### Example output files can be found within the [BIDS documentation](https://bids-specification.readthedocs.io/en/stable/modality-specific-files/electroencephalography.html).\n",
    "> Note: this code uses the sidecar.json file as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc49e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_path1 = BIDSPath(subject='S001', task='task1',\n",
    "                     suffix='eeg', extension='.json', datatype='eeg',\n",
    "                     root=root)\n",
    "\n",
    "entries = {# Simple key-value pair for head circumference (numerical)\n",
    "           'HeadCircumference': 58.0,\n",
    "           # Simple key-value pair for manufacturer model name (written)\n",
    "            'ManufacturerModelName':\"Brain Products actiCHamp\",\n",
    "            # Nested dictionary for software versions (1-level)\n",
    "            'SoftwareVersions' : {\n",
    "                'MNE': \"1.9.0\",\n",
    "                'BIDS': \"1.10.0\",\n",
    "                'MNE-BIDS': \"0.16.0\"\n",
    "                },\n",
    "           # Nested dictionary for software filters (2-levels)\n",
    "           'SoftwareFilters': {\n",
    "                \"Anti-aliasing filter\":{\n",
    "                \"half-amplitude cutoff (Hz)\": 500,\n",
    "                \"Roll-off\": \"6dB/Octave\"\n",
    "                }\n",
    "                },\n",
    "            }   \n",
    "\n",
    "# Update the JSON file with your new entries\n",
    "update_sidecar_json(bids_path1, entries, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca242e97",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5a038",
   "metadata": {},
   "source": [
    "# Sidecar JSON\n",
    ">\n",
    "This file should have a naming format similar to *_eeg.json in your 'eeg' subfolder. \n",
    "\n",
    "MNE-BIDS should automatically generate most of this information, but there may be missing information that's necessary for your dataset.\n",
    "> Take note of any elements that are missing/incorrect; these can be updated using the above section of code.\n",
    ">\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. EEGReference\n",
    "    2. SamplingFrequency\n",
    "    3. PowerlineFrequency\n",
    "    4. SoftwareFilters\n",
    "    5. TaskName\n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    6. TaskDescription\n",
    "    7. Instructions\n",
    "    8. CogAtlasID\n",
    "    9. CogPOID\n",
    "    10. CapManufacturer\n",
    "    11. CapManufacturerModelName\n",
    "    12. SoftwareVersions\n",
    "    13. DeviceSerialNumber\n",
    "    14. EEGChannelCount\n",
    "    15. ECGChannelCount\n",
    "    16. EMGChannelCount\n",
    "    17. EOGChannelCount\n",
    "    18. MISCChannelCount\n",
    "    19. TriggerChannelCount\n",
    "    20. RecordingDuration\n",
    "    21. RecordingType\n",
    "    22. EpochLength\n",
    "    23. EEGGround\n",
    "    24. HeadCircumference\n",
    "    25. EEGPlacementScheme\n",
    "    26. HardwareFilters\n",
    "    27. SubjectArtefactDescription\n",
    "    28. InstitutionName\n",
    "    29. InstitutionAddress\n",
    "    30. InstitutionalDepartment Name\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    31. ElectricalStimulation\n",
    "    32. ElectricalStimulationParameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b34933",
   "metadata": {},
   "source": [
    "# Coordinate System JSON\n",
    "This file should have a naming format similar to *_coordsystem.json in your 'eeg' subfolder. \n",
    "\n",
    "MNE-BIDS should automatically generate most of this information, but there may be missing information that's necessary for your dataset.\n",
    ">Take note of any elements that are missing/incorrect; these can be updated using the next section of code.\n",
    "\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. EEGCoordinateSystem\n",
    "    2. EEGCoordinateUnits\n",
    "    3. EEGCoordinateSystemDescription\n",
    "Recommended:\n",
    ">\n",
    "    4. FiducialsDescription\n",
    "    5. FiducialsCoordinates\n",
    "    6. FiducialsCoordinateSystem\n",
    "    7. FiducialsCoordinateUnits\n",
    "    8. FiducialsCoordinateSystemDescription\n",
    "    9. AnatomicalLandmarkCoordinates\n",
    "    10. AnatomicalLandmarkCoordinateSystem\n",
    "    11. AnatomicalLandmarkCoordinateUnits\n",
    "    12. AnatomicalLandmarkCoordinateSystemDescription\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    13. IntendedFor\n",
    "\n",
    "> Note: This file requires an extra variable, `space=` to be inputted while `task=` must be `None`. This refers to the coordinate system being used, e.g.'CapTrak'.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40624698",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_path1 = BIDSPath(subject='001', task=None,\n",
    "                     suffix='coordsystem', extension='.json', datatype='eeg',\n",
    "                     root=root, space='CapTrak')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91b9da",
   "metadata": {},
   "source": [
    "# Events JSON\n",
    "This file should have a format similar to *_events.json in your 'eeg' subfolder. \n",
    "\n",
    "This will serve as the explanatory counterpart to the events.tsv file. Any edits made to the contents of the tsv file should be mirrored here, with a description.\n",
    "MNE-BIDS will automatically input the majority of this information, but you may wish to edit these descriptions to be more accurate, or add a description for a new entry.\n",
    "> Take note of any elements that are missing/incorrect; these can be updated using the above section of code.\n",
    ">\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. Onset\n",
    "    2. Duration\n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    n/a\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    3. TrialType\n",
    "    4. ResponseTime\n",
    "    5. HED\n",
    "    6. StimFile\n",
    "    7. Channel\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61da025",
   "metadata": {},
   "source": [
    "# Participants JSON\n",
    "The participants.json file exists as a counterpart to the participants.tsv file and is used to describe the TSV column names and the properties of their values, making interpretation easier, especially in the case of dataset-specific columns. \n",
    ">\n",
    "MNE-BIDS will automatically input the majority of this information, but you may wish to edit these descriptions to be more accurate, and should add additional descriptions for each new entry added to the participants.tsv file (e.g. education level) using the above code.\n",
    "> Take note of any elements that are missing/incorrect; these can be updated using the above section of code.\n",
    "\n",
    "#### BIDS Components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. Participant ID \n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    2. Species\n",
    "    3. Age\n",
    "    4. Sex\n",
    "    5. Handedness\n",
    "    6. Strain\n",
    "    7. Strain RRID\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    - Additional participant information may be included to further bolster your metadata.\n",
    "\n",
    "> Note: This file requires the input `None` for the `subject=`, `task=` and `datatype=` sections\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_path1 = BIDSPath(subject=None, task=None,\n",
    "                     suffix='participants', extension='.json', datatype=None,\n",
    "                     root=root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00c67e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2302d75",
   "metadata": {},
   "source": [
    "#### As it requires creation of a whole new folder and file path, this next file, although still including json and tsv files, requires slighhtly different code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50b1491",
   "metadata": {},
   "source": [
    "<a id=\"phenotype\"></a>\n",
    "# Phenotype files\n",
    "> Optional\n",
    "> Datasets with multiple sets of participant level measurements (such as responses from multiple questionnaires) may benefit from being split into files separate from the participants files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aed51ec",
   "metadata": {},
   "source": [
    "The only requirements for these files are that their first column is participant_id, that their rows correspond directly with the subjects in the BIDS dataset, and they have a descriptive name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35d383",
   "metadata": {},
   "source": [
    "First, we must create the folder we want to write the file to. \n",
    "\n",
    "In the file path section, we will be adding \"phenotype\" to the end of our 'root' file pathway to create a new folder named 'phenotype' there. You must then define the file name you wish to create by editing `\"descriptive_file_name.tsv\"` to include a file name that accurately represents the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ceee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the file path for the new folder\n",
    "phenotype_1_folder = Path(root / \"phenotype\")\n",
    "\n",
    "# Creating the new folder\n",
    "phenotype_1_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Assigning the new tsv file path to the new folder location\n",
    "phenotype_tsv_path = phenotype_1_folder / \"descriptive_file_name.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1158eb6",
   "metadata": {},
   "source": [
    "Now, you can add your data to the key:value pairs in the dictionary below to set the contents of the tsv file. \n",
    "\n",
    "The 'keys' will become the column headers, while the 'values' will be the data assigned to each row for that header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the file's contents (column:row entry) using a data frame\n",
    "phenotype_1 = {\n",
    "        'participant_id': [\"Sub-001\", \"Sub-002\"], \n",
    "        'Related_Key': \"Related_Value\"\n",
    "}\n",
    "\n",
    "# Adding the data frame to the new file\n",
    "descriptive_file_name_phenotype = pd.DataFrame(phenotype_1)\n",
    "\n",
    "# Writing the changes to the file\n",
    "descriptive_file_name_phenotype.to_csv(phenotype_tsv_path, sep=\"\\t\", index=False, na_rep=\"n/a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17460014",
   "metadata": {},
   "source": [
    "#### It is recommended that this file is accompanied by a descriptive json file, explaining each of its columns.\n",
    "First, let's create the file path for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the new json file path to the new folder location\n",
    "phenotype_json_path = phenotype_1_folder / \"descriptive_file_name.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e366dd5",
   "metadata": {},
   "source": [
    "Next, edit the 'entries' dictionary to include the description information for every variable entered into the tsv file. This will accept key:value pair formats, including nested dictionaries.\n",
    "\n",
    "We will also import the `json` tool to allow us to create the new json file\n",
    "\n",
    "Then, we will write this to the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the json tool\n",
    "import json\n",
    "\n",
    "# Creating the data entries for the json file\n",
    "entries = { \n",
    "        'participant_id': {\"Description\": \"The participant's unique identifier code\"}, \n",
    "        'Related_Key': {\"Description\": \"An input related to the phenotype file you are creating\"}\n",
    "        }   \n",
    "\n",
    "# Writing the changes to the json file    \n",
    "with open(phenotype_json_path, \"w\") as outfile:\n",
    "    json.dump(entries, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b67aa",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b65722",
   "metadata": {},
   "source": [
    "<a id=\"tsv\"></a>\n",
    "## Manually updating a TSV file:\n",
    "\n",
    "This next section will walk you through editing your TSV files. \n",
    "\n",
    "#### Example output files can be found within the [BIDS documentation](https://bids-specification.readthedocs.io/en/stable/modality-specific-files/electroencephalography.html).\n",
    "> Note: this code uses the participants.tsv file as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd5922",
   "metadata": {},
   "source": [
    "To edit this file, we must first edit the double quotation marks (\"\") to match the full name of your selected tsv file, which will usually be combined with the `eeg_root` variable to create the file path. However, with the participants file, due to its different location in the folder structure, we will substitute this for the `root` variable.\n",
    "\n",
    "This will ensure that the variable `file_name_tsv` refers to the file we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b34d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = root / \"participants.tsv\"\n",
    "# Assigning the .tsv file to a variable\n",
    "file_tsv = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5ebf9",
   "metadata": {},
   "source": [
    "#### Adding/ Editing a column:\n",
    "\n",
    "Both of these functions can be managed using the same section of code!\n",
    "\n",
    "First, you should edit the 'Inputs' list to include the variables you wish to add to your new or pre-existing column. This should be done in row order, beginning with the entry for the first row in the file, and an entry must be submitted for each row.\n",
    "\n",
    "Then, you should change the text in double quotation marks (\"\") within `file_tsv[\"__\"]`, to either title of the pre-existing column you wish to add to, or the title of the new column you wish to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the desired inputs for the chosen column\n",
    "Inputs = [\"High School\", \"A-level\", \"Bachelors\", \"PhD\"]  \n",
    "\n",
    "# Setting the rows in the chosen column to the inputs listed above\n",
    "file_tsv[\"Education\"] = Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e7a3f",
   "metadata": {},
   "source": [
    "#### Editing one row:\n",
    " \n",
    "To edit a single row, you must use the `.loc` function, which allows us to select a row via it's label under the first column. These column and label names will change from file to file, but in this case, we will use the column header participant ID (set in the first double quotation marks), and set the participant ID number (in the second set of double quotation marks) to match that of the row we hope to edit. \n",
    "\n",
    "From there, you can edit the column name to match the one you'd like to edit (this can be different to the column used prior) (in the third set of double quotation marks) and then the item you'd like to assign to the location (in the fourth set of double quotation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing just one row in the .tsv file\n",
    "file_tsv.loc[file_tsv[\"participant_id\"] == \"sub-001\", \"Education\"] = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7bdcfb",
   "metadata": {},
   "source": [
    "#### Removing a row:\n",
    "\n",
    "To remove a row, you must use the `.drop` function, to which you assign an index, which is the number assigned to the row you wish to remove. \n",
    "> Note: Indexes begin from 0, so the 'first' row will be #0, the 'second' row will be #1 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ab43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing a row from the .tsv file using the row's index\n",
    "file_tsv = file_tsv.drop(index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35d1ee",
   "metadata": {},
   "source": [
    "#### Adding a row:\n",
    "\n",
    "To add a row, you must first create a new data frame containing all of the columns and their values (in key:value pairs) that you want to add to the new row (do so by editing the text in the double quotation marks in the first line, and adding new key:value pairs where necessary). The 'key' should relate to the column (and have the same name as an existing one), while the 'value' should relate to the new input.\n",
    "\n",
    "This will then be combined with the current data frame (participants_tsv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4715ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new data frame for the new row\n",
    "new_row = pd.DataFrame([{\"participant_id\": \"sub-002\", \"age\": 30, \"sex\":\"M\"}])\n",
    "\n",
    "# Combining the new row with the existing file_tsv data frame\n",
    "file_tsv = pd.concat([file_tsv, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1f855",
   "metadata": {},
   "source": [
    "Finally, after completing any necessary changes, we __must__ write them back to the file with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed5fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the change to the file\n",
    "file_tsv.to_csv(file_path, sep= '\\t', index=False, na_rep='n/a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfd456",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172f231",
   "metadata": {},
   "source": [
    "# Channels Description TSV\n",
    ">\n",
    "This should have a format similar to *_channels.tsv in your 'eeg' subfolder. \n",
    "\n",
    "Once you have located the file, you should open it and look through the components it lists. Below is a list of information BIDS needs/suggests for this file. Take note of which elements are missing or incorrect.\n",
    ">\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. Name\n",
    "    2. Type\n",
    "    3. Units\n",
    "Recommended\n",
    ">\n",
    "    n/a\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    4. Description\n",
    "    5. SamplingFrequency\n",
    "    6. Reference\n",
    "    7. LowCutoff\n",
    "    8. HighCutoff\n",
    "    9. Notch\n",
    "    10. Status\n",
    "    11. StatusDescription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d72916",
   "metadata": {},
   "source": [
    "# Electrodes Description TSV\n",
    "This should have a format similar to *_electrodes.tsv in your 'eeg' subfolder. \n",
    "\n",
    "Once you have located the file, you should open it and look through the components it lists. Below is a list of information BIDS needs/suggests for this file. Take note of which elements are missing/incorrect.\n",
    ">\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. X\n",
    "    2. Y\n",
    "    3. Z\n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    4. Type\n",
    "    5. Material\n",
    "    6. Impedance\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c7d49",
   "metadata": {},
   "source": [
    "# Events TSV\n",
    "This file's name should have a format similar to *_events.tsv in your 'eeg' subfolder. \n",
    "\n",
    "This is the main dataset file for 'events', containing the table of variables and their values, which are defined in the json file.\n",
    "\n",
    "Once you have located this file, you should open it and look through it's variables. Below is a list of information BIDS needs/suggests for this file. \n",
    "> Take note of any elements that are missing/incorrect; these can be updated using the next section of code.\n",
    ">\n",
    "#### BIDS components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. Onset\n",
    "    2. Duration\n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    n/a\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    3. TrialType\n",
    "    4. ResponseTime\n",
    "    5. HED\n",
    "    6. StimFile\n",
    "    7. Channel\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfeaab",
   "metadata": {},
   "source": [
    "# Participants TSV\n",
    "The participants.tsv file includes a table containing participant information relevant to the dataset. It is accompanied by the participants.json file, which provides more in-depth explanations for this information.\n",
    ">\n",
    "MNE-BIDS will automatically input the majority of this information, but you may wish to edit the file in order to add more columns to include further participant information.\n",
    "\n",
    "#### BIDS Components:\n",
    ">\n",
    "Necessary:\n",
    ">\n",
    "    1. Participant ID \n",
    ">\n",
    "Recommended:\n",
    ">\n",
    "    2. Species\n",
    "    3. Age\n",
    "    4. Sex\n",
    "    5. Handedness\n",
    "    6. Strain\n",
    "    7. Strain RRID\n",
    ">\n",
    "Optional:\n",
    ">\n",
    "    - Additional participant information may be included to further bolster your metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b78a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ace920",
   "metadata": {},
   "source": [
    "<a id=\"validator\"></a>\n",
    "# Validating your BIDS dataset\n",
    "Now that you have completed your edits, we suggest checking your BIDS files against this [BIDS validator](https://bids-standard.github.io/bids-validator/) to check how closely your dataset complies with BIDS formatting, and whether there is anything that may need further adapting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee5b47",
   "metadata": {},
   "source": [
    "# How to cite MNE-BIDS\n",
    "\n",
    "#### As we used their tools to generate our BIDS formatted dataset, we must cite MNE-BIDS somewhere within it!\n",
    "The following code will automatically do this for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980bca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = bids_root / \"README\"\n",
    "with open(readme, encoding=\"utf-8-sig\") as fid:\n",
    "    text = fid.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de753fcd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede764c4",
   "metadata": {},
   "source": [
    "# Our Citations\n",
    "MNE-BIDS\n",
    "> Appelhoff, S., Sanderson, M., Brooks, T., Vliet, M., Quentin, R., Holdgraf, C., Chaumon, M., Mikulan, E., Tavabi, K., Hchenberger, R., Welke, D., Brunner, C., Rockhill, A., Larson, E., Gramfort, A., & Jas, M. (2019). MNE-BIDS: Organizing electrophysiological data into the BIDS format and facilitating their analysis. Journal of Open Source Software, 4:1896. DOI: 10.21105/joss.01896\n",
    ">\n",
    ">  Pernet, C.R., Appelhoff, S., Gorgolewski, K.J. et al. EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Sci Data 6, 103 (2019). https://doi.org/10.1038/s41597-019-0104-8\n",
    ">\n",
    "MNE-Python\n",
    "> Alexandre Gramfort, Martin Luessi, Eric Larson, Denis A. Engemann, Daniel Strohmeier, Christian Brodbeck, Roman Goj, Mainak Jas, Teon Brooks, Lauri Parkkonen, and Matti S. Hmlinen. MEG and EEG data analysis with MNE-Python. Frontiers in Neuroscience, 7(267):113, 2013. doi:10.3389/fnins.2013.00267.\n",
    ">\n",
    "BIDS\n",
    "> Gorgolewski, K.J., Auer, T., Calhoun, V.D., Craddock, R.C., Das, S., Duff, E.P., Flandin, G., Ghosh, S.S., Glatard, T., Halchenko, Y.O., Handwerker, D.A., Hanke, M., Keator, D., Li, X., Michael, Z., Maumet, C., Nichols, B.N., Nichols, T.E., Pellman, J., Poline, J.-B., Rokem, A., Schaefer, G., Sochat, V., Triplett, W., Turner, J.A., Varoquaux, G., Poldrack, R.A. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. Scientific Data, 3 (160044). doi:10.1038/sdata.2016.44\n",
    ">\n",
    ">  Pernet, C. R., Appelhoff, S., Gorgolewski, K.J., Flandin, G., Phillips, C., Delorme, A., Oostenveld, R. (2019). EEG-BIDS, an extension to the brain imaging data structure for electroencephalography. Scientific data, 6 (103). doi:10.1038/s41597-019-0104-8\n",
    ">\n",
    "EEG Motor Movement/Imagery Dataset\n",
    "> Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE Transactions on Biomedical Engineering 51(6):1034-1043, 2004.\n",
    ">\n",
    "PhysioNet\n",
    "> Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215e220. RRID:SCR_007345.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea777d3",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff630dde",
   "metadata": {},
   "source": [
    "### Adapting code to iterate through all participants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499f486",
   "metadata": {},
   "source": [
    "The following block of code is intended to be run in one go. It uses the same code as explained earlier in the pipeline, but has no visualisations and has some added code to allow it to format all of your participant data into BIDS compliancy automatically, with minimal input.\n",
    "\n",
    "To input your participants' information into this code, it is necessary that you locate and edit the MNE_BIDS_sheet excel document, stored within the same ['RosettaState'](https://github.com/ubdbra001/RosettaState/tree/MNE-BIDS-pipeline-post-feedback) repository as the current document. You should first delete the example inputs, then insert the correct information for each participant under the related column name. This should be done for every sheet (subject_info, events, age). This spreadsheet will include formatting examples for each of the sheets.\n",
    "\n",
    "There are some minor edits that you will need to conduct within the code to allow it to run with your dataset. Within the code, you should edit:\n",
    "- The file path for the 'data_dir' variable -> change the text in single quotation marks to the file path for the folder containing your entire dataset.\n",
    "- The file path for the 'xls' variable -> input the file path for your excel spreadsheet (in the single quotation marks)\n",
    "- The date input for `raw.set_meas_date()` -> change the date input to the date you recorded the data on (YYYY/M/D)\n",
    "- The inputs for each of the 'raw.info' items (excluding subject_info) -> edit the second set of double quotation marks or swap out the integer (either after the colon or equals symbol)\n",
    "> \"device_info\"\n",
    "> \"line_freq\"\n",
    "> \"description\"\n",
    "> \"dev_head_t\"\n",
    "> \"experimenter\"\n",
    "- Your dataset's montage -> change the text in the double quotation marks to match the name of the dataset's montage\n",
    "- The variable 'Orig_time' -> the origin time for the dataset, set this to `None` to use meas_date\n",
    "- The task name -> set the text in double quotation marks to match your task name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846284c",
   "metadata": {},
   "source": [
    "#### Subject_info\n",
    "Due to between-participant differences in inputs for the 'subject_info' entry, in order to automatically input the data, we must use a different method. \n",
    "We will use a spreadsheet, in which you must input entries for all of the subject_info variables (in columns) for every participant (one per row) in order, starting with the lowest participant id. The process also requires a 'for' loop, which will input information to the 'info' variable and create a BIDS formatted dataset for each participant's files.\n",
    "\n",
    "The required variables are:\n",
    "- his_id - The string subject identifier\n",
    "- last_name - The participant's last name\n",
    "- first_name - The participant's first name\n",
    "- middle_name - The participants middle name\n",
    "- sex - The biological sex of the participant (0 = unknown, 1 = male, 2 = female)\n",
    "- hand - Whether the participant is right handed (1), left handed (2) or ambidextrous (3)\n",
    "- weight - Weight in kilograms\n",
    "- height - Height in meters\n",
    "\n",
    "This should be done for each of the tasks present (e.g. one run through for 'rest' and one for 'video')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad62440",
   "metadata": {},
   "source": [
    "#### Ages\n",
    "To input participant age into the dataset, you should edit the column of the 'ages' sheet of this spreadsheet to match the ages of all of the participants present, in subject order.\n",
    "\n",
    "We have chosen to use age instead of birthdate, as it is more commonly collected. \n",
    "So, if your dataset uses birthdate instead of age, you may benefit from using the code below to convert your birth dates into ages, using the measuring date of the dataset. To do so, you must simply enter your birth dates into the variable 'dob_list' in participant order, in the format `date(YYYY/M/D)`. This will output a list of ages that you can insert into the 'ages' sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8473340",
   "metadata": {},
   "source": [
    "#### Code for changing birthdate to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aeec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m date\n\u001b[1;32m----> 3\u001b[0m \u001b[43mraw\u001b[49m\u001b[38;5;241m.\u001b[39mset_meas_date(datetime(\u001b[38;5;241m2025\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, tzinfo\u001b[38;5;241m=\u001b[39m timezone\u001b[38;5;241m.\u001b[39mutc))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_age\u001b[39m(dob):\n\u001b[0;32m      6\u001b[0m     recording_date \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeas_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdate()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'raw' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing necessary tools\n",
    "from datetime import date\n",
    "\n",
    "# Setting the measurement date\n",
    "raw.set_meas_date(datetime(2025, 6, 7, tzinfo= timezone.utc))\n",
    "\n",
    "# Function for calculating age, using date of birth and measurement date\n",
    "def calculate_age(dob):\n",
    "    recording_date = raw.info[\"meas_date\"].date()\n",
    "    return recording_date.year - dob.year - ((recording_date.month, recording_date.day) < (dob.month, dob.day))\n",
    "\n",
    "# Create a list of the participants' birthdates by inputting them into the below brackets\n",
    "dob_list = date(2005, 3, 3), date(2000, 6, 8), date(1985, 2, 3)\n",
    "# Calculates age for each participant \n",
    "for age in dob_list:\n",
    "    print(calculate_age(dob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f4688",
   "metadata": {},
   "source": [
    "#### Events \n",
    "\n",
    "To input event information into your dataset, you should edit the 'events' sheet within the excel spreadsheet. \n",
    "\n",
    "The information that will need inputting includes:\n",
    "- Participant -> should match the participant id of the individual\n",
    "- Description -> A short description of the event\n",
    "- Duration -> the duration of the event in seconds\n",
    "- Onset -> the onset time of the event in seconds (relative to origin time)\n",
    "- Ch_names -> the exact name of the channel associated with the event\n",
    "- Event_id -> the event's unique identifier number (to be defined by you)\n",
    "\n",
    "If an event has more than one associated channels, this sheet will require multiple row inputs per participant (see sheet for an example). All inputs should remain the same in the additional rows, except channel name, which should be changed to match the additional channel names. \n",
    "\n",
    "The channel names must exactly match those associated with the dataset, so it may be beneficial to use the line of code below, `raw.info['ch_names']`, to find the full list of exact channel names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb41878",
   "metadata": {},
   "source": [
    "#### Setting missing values\n",
    "In the spreadsheet, all columns must be filled for each participant. To indicate a missing value, any integer inputs (e.g. sex) can be changed to '0', while string (written) inputs (e.g. middle_name) can be changed to a consistent identifier, such as 'missing'. Note: id and his_id cannot be set as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6aa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info['ch_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc637dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne_bids in c:\\users\\ariana\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: mne>=1.7 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.1 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.14.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (3.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (1.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ariana\\lib\\site-packages (from pooch>=1.5->mne>=1.7->mne_bids) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ariana\\lib\\site-packages (from pooch>=1.5->mne>=1.7->mne_bids) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ariana\\lib\\site-packages (from jinja2->mne>=1.7->mne_bids) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariana\\lib\\site-packages (from tqdm->mne>=1.7->mne_bids) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ariana\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne>=1.7->mne_bids) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mne_bids in c:\\users\\ariana\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: mne>=1.7 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.9.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.1 in c:\\users\\ariana\\lib\\site-packages (from mne_bids) (1.14.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (3.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (1.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ariana\\lib\\site-packages (from mne>=1.7->mne_bids) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ariana\\lib\\site-packages (from matplotlib>=3.6->mne>=1.7->mne_bids) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ariana\\lib\\site-packages (from pooch>=1.5->mne>=1.7->mne_bids) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ariana\\lib\\site-packages (from pooch>=1.5->mne>=1.7->mne_bids) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ariana\\lib\\site-packages (from jinja2->mne>=1.7->mne_bids) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ariana\\lib\\site-packages (from tqdm->mne>=1.7->mne_bids) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ariana\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne>=1.7->mne_bids) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ariana\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.7->mne_bids) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Extracting EDF parameters from c:\\N8_internship_code\\Motor_Imaging_Dataset\\S001\\S001R01.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\participants.tsv'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\participants.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\dataset_description.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S001\\eeg\\sub-S001_task-task1_eeg.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S001\\eeg\\sub-S001_task-task1_channels.tsv'...\n",
      "Copying data files to sub-S001_task-task1_eeg.edf\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S001\\sub-S001_scans.tsv'...\n",
      "Wrote c:\\N8_internship_code\\iterate_test\\sub-S001\\sub-S001_scans.tsv entry with eeg\\sub-S001_task-task1_eeg.edf.\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\participants.tsv'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\participants.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\dataset_description.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S002\\eeg\\sub-S002_task-task1_eeg.json'...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S002\\eeg\\sub-S002_task-task1_channels.tsv'...\n",
      "Copying data files to sub-S002_task-task1_eeg.edf\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Writing 'c:\\N8_internship_code\\iterate_test\\sub-S002\\sub-S002_scans.tsv'...\n",
      "Wrote c:\\N8_internship_code\\iterate_test\\sub-S002\\sub-S002_scans.tsv entry with eeg\\sub-S002_task-task1_eeg.edf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:116: RuntimeWarning: Omitted 2 annotation(s) that were outside data range.\n",
      "  raw.set_annotations(annotations)\n",
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:130: RuntimeWarning: No events found or provided. Please add annotations to the raw data, or provide the events and event_id parameters. For resting state data, BIDS recommends naming the task using labels beginning with \"rest\".\n",
      "  write_raw_bids(raw, bids_path, events, event_id, overwrite=True, allow_preload=True, format=\"EDF\")\n",
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:130: RuntimeWarning: Converting data files to EDF format\n",
      "  write_raw_bids(raw, bids_path, events, event_id, overwrite=True, allow_preload=True, format=\"EDF\")\n",
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:116: RuntimeWarning: Omitted 1 annotation(s) that were outside data range.\n",
      "  raw.set_annotations(annotations)\n",
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:130: RuntimeWarning: No events found or provided. Please add annotations to the raw data, or provide the events and event_id parameters. For resting state data, BIDS recommends naming the task using labels beginning with \"rest\".\n",
      "  write_raw_bids(raw, bids_path, events, event_id, overwrite=True, allow_preload=True, format=\"EDF\")\n",
      "C:\\Users\\Ariana Williams\\AppData\\Local\\Temp\\ipykernel_7120\\1710751408.py:130: RuntimeWarning: Converting data files to EDF format\n",
      "  write_raw_bids(raw, bids_path, events, event_id, overwrite=True, allow_preload=True, format=\"EDF\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Generates the (approximate) birthdate of the participant based on the measurement date and age\u001b[39;00m\n\u001b[0;32m     60\u001b[0m recording_date \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeas_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 61\u001b[0m birthday \u001b[38;5;241m=\u001b[39m recording_date \u001b[38;5;241m-\u001b[39m relativedelta(years\u001b[38;5;241m=\u001b[39m\u001b[43mages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Sets the 'subject info' metadata to the data for the current participant from the spreadsheet list, and sets the birthday using 'birthdate'\u001b[39;00m\n\u001b[0;32m     64\u001b[0m raw\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msubject_dict[index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbirthday\u001b[39m\u001b[38;5;124m\"\u001b[39m: birthday}\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Importing tools\n",
    "import mne\n",
    "\n",
    "from mne_bids import BIDSPath, write_raw_bids\n",
    "import os.path as op\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mne.transforms import Transform\n",
    "\n",
    "# Setting overall file location [edit this]\n",
    "data_dir = Path(r'c:\\N8_internship_code\\Motor_Imaging_Dataset')\n",
    "\n",
    "# Specifying the files\n",
    "children = [child for child in data_dir.iterdir()]\n",
    "dir_number = 5\n",
    "files = [file for file in children[dir_number].iterdir()]\n",
    "file_path = files[1]\n",
    "\n",
    "# Reading the files\n",
    "raw = mne.io.read_raw_edf(file_path)\n",
    "\n",
    "# Making a list of (only) participant folders from the 'location' file path\n",
    "folders = [p_folder for p_folder in data_dir.iterdir() if p_folder.is_dir()]\n",
    "\n",
    "# Load each sheet from your excel file into an individual data frame [edit the file pathway to match yours]\n",
    "xls = pd.ExcelFile(r'c:\\N8_internship_code\\MNE_BIDS_sheet.xlsx')\n",
    "subject_sheet = pd.read_excel(xls, 'subject_info')\n",
    "events_sheet = pd.read_excel(xls, 'events')\n",
    "age_sheet = pd.read_excel(xls, 'ages')\n",
    "\n",
    "# Turns the 'subject_sheet' data frame (created from our excel sheet) into a list of dictionaries (one per participant), containing key:value pairs\n",
    "# Keys = Excel columns, Values = The column's data input\n",
    "subject_dict = subject_sheet.to_dict(orient='records')\n",
    "\n",
    "# Turns the ages from the 'age' excel sheet into a list\n",
    "ages = age_sheet['ages'].tolist()\n",
    "\n",
    "# sets the measurement date of the dataset [edit this to match your own (YYYY, M, D)]\n",
    "raw.set_meas_date(datetime(2025, 6, 7, tzinfo= timezone.utc))\n",
    "\n",
    "# Turns the 'sheet' data frame (created from our excel sheet) into a list of dictionaries (one per participant), containing key:value pairs\n",
    "events_dict = events_sheet.to_dict('list')\n",
    "\n",
    "# Creating the output file location for the dataset\n",
    "bids_root = op.join(data_dir.parent, \"iterate_test\")\n",
    "\n",
    "# Looping through every participant folder and its index in the folders list, using enumerate\n",
    "for index, participant_folder in enumerate(folders):\n",
    "    # Collecting the file name (also participant id) from the file\n",
    "    file_name = participant_folder.name\n",
    "\n",
    "    # Generates the (approximate) birthdate of the participant based on the measurement date and age\n",
    "    recording_date = raw.info[\"meas_date\"]\n",
    "    birthday = recording_date - relativedelta(years=ages[index])\n",
    "\n",
    "    # Sets the 'subject info' metadata to the data for the current participant from the spreadsheet list, and sets the birthday using 'birthdate'\n",
    "    raw.info[\"subject_info\"] = {**subject_dict[index], \"birthday\": birthday}\n",
    "\n",
    "    # Setting the dataset's metadata manually (inputs will often be the same for all participants)\n",
    "    raw.info[\"device_info\"] = {\n",
    "        \"type\": \"EEG\",\n",
    "        \"model\": \"12-channel EEG\",\n",
    "        \"serial\": 33456423\n",
    "        }\n",
    "    raw.info[\"line_freq\"] = 50\n",
    "    raw.info[\"description\"] = \"a resting state dataset\"\n",
    "    raw.info[\"dev_head_t\"] = Transform(\"meg\", \"head\")\n",
    "    raw.info[\"experimenter\"] = \"John Doe\"\n",
    "    my_montage = mne.channels.make_standard_montage(\"biosemi64\")\n",
    "\n",
    "    # Creates a list of numbers that serve as the index for each row of information relating to the current participant\n",
    "    new_list = []\n",
    "    new_list = events_dict['participant']\n",
    "    numbers = []\n",
    "    for index, p_num in enumerate(new_list):\n",
    "        if p_num == file_name:\n",
    "            numbers.append(index)\n",
    "\n",
    "    # Creates a list to hold the annotation inputs, then sets a different list to the excel column for the item\n",
    "    des_list = []\n",
    "    description_list = events_dict['description']\n",
    "    dur_list = []\n",
    "    duration_list = events_dict['duration']\n",
    "    ons_list = []\n",
    "    onset_list = events_dict['onset']\n",
    "    ch_list = []\n",
    "    channel_list = events_dict['ch_names']\n",
    "    id_list = []\n",
    "    event_id_list = events_dict['event_id']\n",
    "\n",
    "    # Collects the information for each event (relating to the current participant) and turns it into lists of event information for each column\n",
    "    for number in numbers:\n",
    "        description = description_list[number]\n",
    "        des_list.append(description)\n",
    "        duration = duration_list[number]\n",
    "        dur_list.append(duration)\n",
    "        onset = onset_list[number]\n",
    "        ons_list.append(onset)\n",
    "        channels = channel_list[number]\n",
    "        ch_list.append(channels)\n",
    "        ids = event_id_list[number]\n",
    "        id_list.append(ids)\n",
    "\n",
    "    # Sets the origin_time [to use meas_date, change this to equal 'None']\n",
    "    orig_time = datetime(1975, 3, 4, tzinfo=timezone.utc)\n",
    "\n",
    "    # Collating and setting the annotation lists to 'raw'\n",
    "    annotations = mne.Annotations(onset=ons_list, duration=dur_list , description=des_list, orig_time=orig_time, ch_names=[[ch]for ch in ch_list])\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # The id associated with each event (via its description)\n",
    "    event_id = dict(zip(des_list, id_list))\n",
    "\n",
    "    # Generates events from 'annotations'\n",
    "    events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "\n",
    "    # Setting participant id, task name should be inputted manually\n",
    "    subject_id = file_name\n",
    "    task = \"task1\"\n",
    "\n",
    "    # Writing the BIDS dataset\n",
    "    bids_path = BIDSPath(subject=subject_id, task=task, root=bids_root)\n",
    "    write_raw_bids(raw, bids_path, events, event_id, overwrite=True, allow_preload=True, format=\"EDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ch_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.info[\"bads\"] = [\"\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee007f",
   "metadata": {},
   "source": [
    "Can then run through more specific edits [LINK TO THE SECTION] if necessary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
